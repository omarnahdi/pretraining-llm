{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Data Preparation\n",
    "\n",
    "In this lesson you'll carry out some of the data cleaning steps required to prepare data for pretraining. In the video, Sung mentioned an Upstage tool called **Dataverse** which can help you with data cleaning. You can checkout the features of Dataverse at [this link](https://github.com/UpstageAI/dataverse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sourcing datasets for pretraining\n",
    "\n",
    "In this section, you'll see two ways to source data for training:\n",
    "1. Download an existing dataset from Hugging Face\n",
    "2. Create a dataset of python scripts sourced from Github\n",
    "\n",
    "In both cases the result will be a Hugging Face `Dataset` object, part of the `Datasets` library. You can read more about the properties of Datasets and how to work with them on the [Hugging Face website](https://huggingface.co/docs/datasets/en/index).\n",
    "\n",
    "### Download data from Hugging face\n",
    "\n",
    "The dataset you download here is a subset of a much larger dataset called **Red Pajama**. The full, 1 trillion token dataset is available on Hugging Face at [this link](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'meta'],\n",
      "    num_rows: 60000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "pretraining_dataset = datasets.load_dataset(\n",
    "    \"upstage/Pretraining_Dataset\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "print(pretraining_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia beats China, works toward delaying Team USA rematch until gold-medal game\n",
      "By Dan FeldmanAug 12, 2016, 3:07 PM EDT\n",
      "AP Photo/Charlie Neibergall\n",
      "Andrew Bogut didn't play. Patty Mills lost his spot as the Rio Olympics' leading scorer.\n",
      "And Australia cruised to a 93-68 win over China in men's basketball group play Friday.\n",
      "Australia (3-1) is now another step closer to second place in Group A, which could mean avoiding a rematch with Team USA until the gold-medal game. Barring an upset – Australia to Venezuela on Sunday, Team USA to Serbia today or France on Sunday – the Americans and Australians will land on opposite sides of the bracket.\n",
      "With no medals in its history, Australia is clearly aiming for more than a few feel-good games. Hence, resting Bogut. Cameron Bairstow (17 points and nine rebounds) and Aron Baynes (12 points and four rebounds) stepped up in the center's absence, as Mills (five points) played but also handled a reduced workload.\n",
      "Yi Jianlian matched Mills for the Rio individual points lead. Displaying the skills that once tricked scouts into believing he had lottery talent, Jianlian (20 points, nine rebounds, two assists and two blocks) played a dazzling all-around game. But China (0-4) needs a miracle set of outcomes, including beating Serbia on Sunday, to advance from the group stage.\n"
     ]
    }
   ],
   "source": [
    "print(pretraining_dataset['text'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'redpajama_set_name': 'RedPajamaC4'}\n"
     ]
    }
   ],
   "source": [
    "print(pretraining_dataset['meta'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only work with the `text` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia beats China, works toward delaying Team USA rematch until gold-medal game\n",
      "By Dan FeldmanAug 12, 2016, 3:07 PM EDT\n",
      "AP Photo/Charlie Neibergall\n",
      "Andrew Bogut didn't play. Patty Mills lost his spot as the Rio Olympics' leading scorer.\n",
      "And Australia cruised to a 93-68 win over China in men's basketball group play Friday.\n",
      "Australia (3-1) is now another step closer to second place in Group A, which could mean avoiding a rematch with Team USA until the gold-medal game. Barring an upset – Austr\n"
     ]
    }
   ],
   "source": [
    "pretraining_dataset = pretraining_dataset.select_columns(\n",
    "    [\"text\"]\n",
    ")\n",
    "\n",
    "# Print a sample:\n",
    "\n",
    "print(pretraining_dataset[3][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare pretraining and fine-tuning datasets\n",
    "In the next cell, you'll download a fine-tuning dataset to contrast with the pretraining dataset you loaded above. You can read more about the Alpaca model and instruction tuning dataset [here](https://crfm.stanford.edu/2023/03/13/alpaca.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output'],\n",
      "    num_rows: 52002\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "instruction_dataset = datasets.load_dataset(\n",
    "    \"c-s-ale/alpaca-gpt4-data\",\n",
    "    split='train'\n",
    ")\n",
    "print(instruction_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Evaluate this sentence for spelling and grammar mistakes\n",
      "Input: He finnished his meal and left the resturant\n",
      "Output: There are two spelling errors in the sentence. The corrected sentence should be: \"He finished his meal and left the restaurant.\"\n"
     ]
    }
   ],
   "source": [
    "i=9\n",
    "print(\"Instruction: \" + instruction_dataset[i][\"instruction\"] \n",
    "      + \"\\nInput: \" + instruction_dataset[i][\"input\"] \n",
    "      + \"\\nOutput: \" + instruction_dataset[i][\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how in contrast to the pretraining data, which is just raw text, fine-tuning datasets are structured into question-answer pairs or instruction-response sets that can include additional input context if required. \n",
    "\n",
    "Moving forward, you'll only work with the unstructured pretraining dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape python code from Github\n",
    "Here, you'll download a selection of python scripts from Github and then prepare them as a Hugging Face `Dataset` object to use in training. \n",
    "\n",
    "The same pattern here will work for preparing any text scraped from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some required packages\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Path to directory to store python scripts\n",
    "code_dir = \"./code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/TheAlgorithms/Python/master/searches/double_linear_search_recursion.py\",\n",
    "    \"https://raw.githubusercontent.com/KosingZhu/tensorflow/master/tensorflow/python/tools/module_util.py\",\n",
    "    \"https://raw.githubusercontent.com/EricRemmerswaal/tensorflow/master/tensorflow/python/distribute/distribute_coordinator_context.py\",\n",
    "    \"https://raw.githubusercontent.com/computationalartist/tensorflow/master/tensorflow/python/ops/numpy_ops/integration_test/benchmarks/numpy_mlp.py\",\n",
    "    \"https://raw.githubusercontent.com/Van-an/tensorflow/master/tensorflow/python/distribute/coordinator/values.py\",\n",
    "    \"https://raw.githubusercontent.com/nkgwer/tensorflow/master/tensorflow/lite/tools/visualize.py\",\n",
    "    \"https://raw.githubusercontent.com/gitblazer/youtube-dl/master/youtube_dl/version.py\",\n",
    "    \"https://raw.githubusercontent.com/Joshua-Barawa/My-Photos/master/venv/lib/python3.8/site-packages/django/contrib/messages/__init__.py\",\n",
    "    \"https://raw.githubusercontent.com/PaliC/pytorch/master/test/fx/test_subgraph_rewriter.py\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on url: https://raw.githubusercontent.com/TheAlgorithms/Python/master/searches/double_linear_search_recursion.py\n",
      "Working on url: https://raw.githubusercontent.com/KosingZhu/tensorflow/master/tensorflow/python/tools/module_util.py\n",
      "Working on url: https://raw.githubusercontent.com/EricRemmerswaal/tensorflow/master/tensorflow/python/distribute/distribute_coordinator_context.py\n",
      "Working on url: https://raw.githubusercontent.com/computationalartist/tensorflow/master/tensorflow/python/ops/numpy_ops/integration_test/benchmarks/numpy_mlp.py\n",
      "Working on url: https://raw.githubusercontent.com/Van-an/tensorflow/master/tensorflow/python/distribute/coordinator/values.py\n",
      "Working on url: https://raw.githubusercontent.com/nkgwer/tensorflow/master/tensorflow/lite/tools/visualize.py\n",
      "Working on url: https://raw.githubusercontent.com/gitblazer/youtube-dl/master/youtube_dl/version.py\n",
      "Working on url: https://raw.githubusercontent.com/Joshua-Barawa/My-Photos/master/venv/lib/python3.8/site-packages/django/contrib/messages/__init__.py\n",
      "Working on url: https://raw.githubusercontent.com/PaliC/pytorch/master/test/fx/test_subgraph_rewriter.py\n"
     ]
    }
   ],
   "source": [
    "for url in urls:\n",
    "    print(f\"Working on url: {url}\")\n",
    "    response = requests.get(url)\n",
    "    file_name = os.path.basename(url)\n",
    "    file_path = os.path.join(code_dir, file_name)\n",
    "    \n",
    "    with open(file_path, \"wb\") as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribute_coordinator_context.py\n",
      "double_linear_search_recursion.py\n",
      "module_util.py\n",
      "numpy_mlp.py\n",
      "test_subgraph_rewriter.py\n",
      "values.py\n",
      "version.py\n",
      "visualize.py\n",
      "__init__.py\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(code_dir)\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate scripts into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dataset = []\n",
    "for file in os.listdir(code_dir):\n",
    "    code_dataset.append(\n",
    "        {'text': open(os.path.join(code_dir, file), 'r').read()}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def search(list_data: list, key: int, left: int = 0, right: int = 0) -> int:\\n    \"\"\"\\n    Iterate thr'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_dataset[1]['text'][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert list to Hugging Face `Dataset` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 9\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "code_dataset = datasets.Dataset.from_list(code_dataset)\n",
    "print(code_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the python code dataset with the pretraining dataset you downloaded above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 60009\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.concatenate_datasets(\n",
    "    [pretraining_dataset, code_dataset]\n",
    ")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data cleaning\n",
    "\n",
    "In the cells below, you'll carry out the following cleaning steps:\n",
    "1. Filter out samples that are too short\n",
    "2. Remove repetitions within a single text example\n",
    "3. Remove duplicated documents\n",
    "4. Quality filter to remove non-English texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60009"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove examples that are too short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def paragraph_length_filter(x):\n",
    "    \"\"\"Returns False iff a page has too few lines or lines are too short.\"\"\"\n",
    "    lines = x['text'].split('\\n')\n",
    "    if (\n",
    "        len(lines) < 3\n",
    "        or min(heapq.nlargest(3, [len(line) for line in lines])) < 3\n",
    "    ):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16efe9b1c5874d45866246c297ae7372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/60009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.filter(\n",
    "    paragraph_length_filter,\n",
    "    load_from_cache_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52356"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove repeated text within training examples\n",
    "\n",
    "Here you'll remove text repetitions within each example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(paragraphs):\n",
    "    \"\"\"\n",
    "    Use this function to find the number of repetitions \n",
    "    in the paragraphs.\n",
    "    \"\"\"\n",
    "    unique_x = set()\n",
    "    duplicate_chars = 0\n",
    "    duplicate_elements = 0\n",
    "    for element in paragraphs:\n",
    "        if element in unique_x:\n",
    "            duplicate_chars += len(element)\n",
    "            duplicate_elements += 1\n",
    "        else:\n",
    "            unique_x.add(element)\n",
    "    return duplicate_elements, duplicate_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def paragraph_repetition_filter(x):\n",
    "    \"\"\"\n",
    "    Returns False iff a page has too many repetitions.\n",
    "    \"\"\"\n",
    "    text = x['text']\n",
    "    paragraphs = re.compile(r\"\\n{2,}\").split(text.strip())                # Split by paragraphs (2 or more newlines)\n",
    "    paragraphs_duplicates, char_duplicates = find_duplicates(paragraphs)  # Find number of duplicates in paragraphs\n",
    "    if paragraphs_duplicates / len(paragraphs) > 0.3:\n",
    "        return False\n",
    "    if char_duplicates / len(text) > 0.2:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3cfb5647751404abaf0ec1f928d2f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/52356 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.filter(\n",
    "    paragraph_repetition_filter,\n",
    "    load_from_cache_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52326"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplication\n",
    "\n",
    "In this section, you'll remove duplicate examples from the entire dataset (in contrast to the previous step where you were just looking for repeated text in each example.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebac478714354fe4889bdf92a5bc7f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/52326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def deduplication(ds):\n",
    "    def dedup_func(x):\n",
    "        \"\"\"Use this function to remove duplicate entries\"\"\"\n",
    "        if x['text'] in unique_text:\n",
    "            return False\n",
    "        else:\n",
    "            unique_text.add(x['text'])\n",
    "            return True\n",
    "\n",
    "    unique_text = set()\n",
    "\n",
    "    ds = ds.filter(dedup_func, load_from_cache_file=False, num_proc=1)\n",
    "    return ds\n",
    "\n",
    "dataset = deduplication(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43597"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality filter - Language\n",
    "\n",
    "Here you'll remove any text examples that are in a language other than English. The code here uses a language detection model called fastText. You can read about fastText [here](https://fasttext.cc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function english_language_filter.<locals>.is_english at 0x000001ED72591B20> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdea77d11874272a26077f635e036c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/43597 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import urllib\n",
    "from fasttext.FastText import _FastText\n",
    "\n",
    "def english_language_filter(ds):\n",
    "    # load language detection model\n",
    "    model = _FastText('./models/upstage/L2_language_model.bin')\n",
    "    \n",
    "    def is_english(x):\n",
    "        # Predict language of the text and probability\n",
    "        language, score = model.predict(x['text'].replace(\"\\n\", \"\"))\n",
    "\n",
    "        language = language[0].split(\"__\")[2]\n",
    "        return score > 0.4 and language == \"en\" # change code here if building a model in another language\n",
    "\n",
    "    ds = ds.filter(is_english, load_from_cache_file=False, num_proc=1)\n",
    "    return ds\n",
    "\n",
    "dataset = english_language_filter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40473"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save the dataset to disk\n",
    "\n",
    "Read more about the parquet data format [here](https://parquet.apache.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632ec2a17ca54211a2c67e358a0ea5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/41 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "197100832"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./data/preprocessed_dataset.parquet\"\n",
    "dataset.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretrain-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
