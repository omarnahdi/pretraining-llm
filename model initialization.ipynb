{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Preparing your model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore insignificant warnings (ex: deprecation warnings)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set a seed value for reproducibility\n",
    "import torch\n",
    "\n",
    "def fix_torch_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_torch_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model configuration\n",
    "\n",
    "You'll configure models based on Meta's Llama family of models. The transformers library has several tools for working with these models, which you can read about [here](https://huggingface.co/docs/transformers/main/en/model_doc/qwen2).\n",
    "\n",
    "Start by creating a `Qwen2Config` object to configure the architecture of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Config {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 22016,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2Config, LlamaConfig\n",
    "\n",
    "llama = LlamaConfig()\n",
    "config = Qwen2Config()\n",
    "\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Config {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config.num_hidden_layers = 12       # reduced from 32 to 12\n",
    "config.hidden_size = 1024           # reduced 1/4 from 4096 to 1024\n",
    "config.intermediate_size = 4096     # reduced 1/3 from 22016 to 4096 (dimension of MLP representations)\n",
    "config.num_key_value_heads = 8      # reduced 1/4 from 32 to 8 (defaults to num_attention_heads=32)\n",
    "config.torch_dtype = \"bfloat16\"     # for half-precision training\n",
    "config.use_cache = False            # 'True' is incompitable w/ gradient checkpointing\n",
    "config.model_type\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weight initialization\n",
    "\n",
    "In the next sections, you'll explore four different ways to initialize the weights of a model for training:\n",
    "1. Random weight initialization\n",
    "2. Using an existing model for continued pre-training\n",
    "3. Downscaling an existing model\n",
    "4. Upscaling an existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random weight initialization\n",
    "\n",
    "Randomly initializing model weights sets all weights to values from a truncated normal distribution with mean 0 and standard deviation of 0.02. Values beyond  2-sigma from the mean are set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (k_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2ForCausalLM\n",
    "model = Qwen2ForCausalLM(config)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters is: 493661184\n"
     ]
    }
   ],
   "source": [
    "def print_nparams(model):\n",
    "    '''Calculate the total number of model parameters'''\n",
    "    nparams = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total number of parameters is: {nparams}\")\n",
    "\n",
    "print_nparams(model) # 493661184 -> 493M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at a sample of the weights in a single layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 30 weights of layer 'model.layers.0.self_attn.q_proj.weight': \n",
      "tensor([ 1.9314e-02, -1.2728e-02, -3.3356e-02, -5.4111e-03, -2.0671e-05,\n",
      "         7.3875e-03, -1.1259e-02,  8.7001e-03,  2.5883e-02, -1.0809e-02,\n",
      "        -8.6876e-04, -1.3179e-02, -4.6141e-03,  2.0350e-02, -3.2663e-02,\n",
      "         2.6889e-02, -2.3778e-02, -1.4012e-02, -3.5504e-02,  6.4026e-03,\n",
      "         1.5647e-02,  6.7447e-03,  1.2999e-03, -4.6823e-03,  6.5911e-04,\n",
      "        -5.3404e-03, -1.8712e-02,  3.2474e-02,  1.0103e-02, -7.9912e-03])\n"
     ]
    }
   ],
   "source": [
    "layer_name = \"model.layers.0.self_attn.q_proj.weight\"\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name == layer_name:\n",
    "        print(f\"First 30 weights of layer '{layer_name}': \")\n",
    "        print(param.data.view(-1)[:30])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using the model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " _('设置了 _('设置了 _('设置了 _('在传奇失落CBS设置了 @$ 이제 @$🌌 @$🌌 @$🌌 @$🌌 @$ @$ @$ @$ @$.DropDownList casualty @$ @$ @$.DropDownList casualty @$.DropDownList casualty @$.DropDownList casualty @$ manners истории истории истории истории истории истории истории истории истории истории истории истории истории истории истории истории истории истории историиynchronized grossoutcome grossoutcomeMitchoutcomeMitchoutcomeMitchoutcomeMitchoutcomeMitchoutcomeMitchoutcomeMitchoutcomeMitchoutcomeMitchoutcomeMitchoutcomeMitchoutcome истории истории истории историиoutcomeoutcomeoutcomeoutcomeoutcomeoutcomeoutcomeoutcomeoutcomeoutcomeoutcomeoutcomeoutcomeoutcomeoutcomeoutcomeoutcomeoutcomeoutcomeoutcomeoutcome圄outcome Detected Detected Detected Detectedoutcome Detected Detected Detected Detectedoutcome Detectedoutcomeoutcomeoutcome\n"
     ]
    }
   ],
   "source": [
    "# Load a tokenizer from Qwen2\n",
    "from transformers import Qwen2Tokenizer\n",
    "tokenizer = Qwen2Tokenizer.from_pretrained(\"Qwen/Qwen-tokenizer\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "prompt = \"I am an AI Developer. I love\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the model from memory to avoid crashing the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\n",
    "import gc\n",
    "del model\n",
    "del streamer\n",
    "del output\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reuse general pretrained model weights\n",
    "\n",
    "If you load an existing model, you can use it as is to continue pretraining on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters is: 248013824\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name_or_path = \"upstage/TinySolar-248m-4k\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,    \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "print_nparams(model) #  248013824 => 248M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downscaling from a general pretrained model\n",
    "\n",
    "Here you'll downscale the HuggingFaceTB/SmolLM2-360M-Instruct model from a 12 layer model to a 10 layer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the middle two layers (layers 15 and 16) and update the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters is: 217601024\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "layers = model.model.layers\n",
    "model.model.layers = layers[:5] + layers[-5:]\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,    \n",
    "    num_hidden_layers=len(model.model.layers),\n",
    ")\n",
    "model.config = config\n",
    "\n",
    "print_nparams(model)  # 217601024 => 217M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear the memory to avoid crashing the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\n",
    "import gc\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth Upscaling from a general pretrained model\n",
    "\n",
    "Here you are going to upscale the HuggingFaceTB/SmolLM2-360M-Instruct model from 12 layers to 16 layers. Here are the steps you'll take:\n",
    "1. Configure a 16 layer model and initialize it with random weights\n",
    "2. Load the 12 layer HuggingFaceTB/SmolLM2-360M-Instruct model into memory\n",
    "3. Copy the bottom 8 and top 8 layers from the 12 layer model and use them to overwrite the random weights of the 16 layer model\n",
    "4. Copy over the embedding and classifying layers to replace the randomly initialized counterparts in the 16 layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = LlamaConfig(\n",
    "#     num_hidden_layers=16,  # We want our model to have 16 final layers\n",
    "#     hidden_size=1024,\n",
    "#     intermediate_size=4096,\n",
    "#     num_attention_heads=32,\n",
    "#     num_key_value_heads=8,\n",
    "#     torch_dtype=\"bfloat16\",\n",
    "#     use_cache=False \n",
    "# )\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Config {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = Qwen2Config(\n",
    "    num_hidden_layers=16,  # We want our model to have 16 final layers\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=4096,\n",
    "    num_attention_heads=32,\n",
    "    num_key_value_heads=8,\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    use_cache=False \n",
    ")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters is: 554492928\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Qwen2ForCausalLM(config)\n",
    "model = model.to(dtype=torch.bfloat16) # convert to bfloat16\n",
    "print_nparams(model) # 554492928 -> 554M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters is: 248013824\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"upstage/TinySolar-248m-4k\"\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,    \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "print_nparams(pretrained_model) #  248013824 => 248M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "model.model.layers = deepcopy(pretrained_model.model.layers[:-4]) \\\n",
    "    + deepcopy(pretrained_model.model.layers[4:])\n",
    "\n",
    "model.model.embed_tokens = deepcopy(pretrained_model.model.embed_tokens)\n",
    "\n",
    "model.lm_head = deepcopy(pretrained_model.lm_head)\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters is: 308839424\n"
     ]
    }
   ],
   "source": [
    "print_nparams(model) # 308839424 -> 308M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the way you can use the data to make a better and more efficient use of your time.\n",
      "\n",
      "A: I've been using the data-mining-in-the-future-of-the-difference-between-the-difference-between-the-difference-between-the-difference-between-the-difference-between-the-difference-between-the-difference-between-the-difference-and-the-the-difference-between-the-difference-and-the-the-difference-between-the-difference-\n"
     ]
    }
   ],
   "source": [
    "# Load a tokenizer from Qwen2\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "prompt = \"I am an AI Developer. I love\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model to disk\n",
    "\n",
    "Note the new model name here which reflects the 308 million parameters of the new, upscaled model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./data/TinySolar-308m-4k-init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretrain-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
