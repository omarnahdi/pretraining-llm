{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6. Model evaluation\n",
    "\n",
    "The model comparison tool that Sung described in the video can be found at this link: https://console.upstage.ai/ (note that you need to create a free account to try it out.)\n",
    "\n",
    "A useful tool for evaluating LLMs is the **LM Evaluation Harness** built by EleutherAI. Information about the harness can be found at this [github repo](https://github.com/EleutherAI/lm-evaluation-harness):\n",
    "\n",
    "You can run the commented code below to install the evaluation harness in your own environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/EleutherAI/lm-evaluation-harness\n",
      "  Cloning https://github.com/EleutherAI/lm-evaluation-harness to c:\\users\\infinix\\appdata\\local\\temp\\pip-req-build-uj1xxbsc\n",
      "  Resolved https://github.com/EleutherAI/lm-evaluation-harness to commit 0f944779d5f37f74480664da71a4ee301f62d013\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from lm_eval==0.4.8) (0.26.1)\n",
      "Collecting evaluate (from lm_eval==0.4.8)\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.16.0 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from lm_eval==0.4.8) (2.16.1)\n",
      "Collecting jsonlines (from lm_eval==0.4.8)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting numexpr (from lm_eval==0.4.8)\n",
      "  Downloading numexpr-2.10.2-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting peft>=0.2.0 (from lm_eval==0.4.8)\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pybind11>=2.6.2 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from lm_eval==0.4.8) (2.13.6)\n",
      "Collecting pytablewriter (from lm_eval==0.4.8)\n",
      "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting rouge-score>=0.0.4 (from lm_eval==0.4.8)\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting sacrebleu>=1.5.0 (from lm_eval==0.4.8)\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting scikit-learn>=0.24.1 (from lm_eval==0.4.8)\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting sqlitedict (from lm_eval==0.4.8)\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch>=1.8 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from lm_eval==0.4.8) (2.1.2)\n",
      "Collecting tqdm-multiprocess (from lm_eval==0.4.8)\n",
      "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: transformers>=4.1 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from lm_eval==0.4.8) (4.49.0)\n",
      "Collecting zstandard (from lm_eval==0.4.8)\n",
      "  Downloading zstandard-0.23.0-cp311-cp311-win_amd64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: dill in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from lm_eval==0.4.8) (0.3.7)\n",
      "Collecting word2number (from lm_eval==0.4.8)\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting more_itertools (from lm_eval==0.4.8)\n",
      "  Downloading more_itertools-10.6.0-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (0.28.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (0.5.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from datasets>=2.16.0->lm_eval==0.4.8) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from datasets>=2.16.0->lm_eval==0.4.8) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from datasets>=2.16.0->lm_eval==0.4.8) (0.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from datasets>=2.16.0->lm_eval==0.4.8) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from datasets>=2.16.0->lm_eval==0.4.8) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from datasets>=2.16.0->lm_eval==0.4.8) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from datasets>=2.16.0->lm_eval==0.4.8) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from datasets>=2.16.0->lm_eval==0.4.8) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from datasets>=2.16.0->lm_eval==0.4.8) (3.11.12)\n",
      "Collecting absl-py (from rouge-score>=0.0.4->lm_eval==0.4.8)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting nltk (from rouge-score>=0.0.4->lm_eval==0.4.8)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from rouge-score>=0.0.4->lm_eval==0.4.8) (1.17.0)\n",
      "Collecting portalocker (from sacrebleu>=1.5.0->lm_eval==0.4.8)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.8) (2024.11.6)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu>=1.5.0->lm_eval==0.4.8)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from sacrebleu>=1.5.0->lm_eval==0.4.8) (0.4.6)\n",
      "Collecting lxml (from sacrebleu>=1.5.0->lm_eval==0.4.8)\n",
      "  Downloading lxml-5.3.1-cp311-cp311-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn>=0.24.1->lm_eval==0.4.8)\n",
      "  Downloading scipy-1.15.2-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn>=0.24.1->lm_eval==0.4.8)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=0.24.1->lm_eval==0.4.8)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from torch>=1.8->lm_eval==0.4.8) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from torch>=1.8->lm_eval==0.4.8) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from torch>=1.8->lm_eval==0.4.8) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from torch>=1.8->lm_eval==0.4.8) (3.1.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from transformers>=4.1->lm_eval==0.4.8) (0.21.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from jsonlines->lm_eval==0.4.8) (25.1.0)\n",
      "Requirement already satisfied: setuptools>=38.3.0 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from pytablewriter->lm_eval==0.4.8) (65.5.0)\n",
      "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm_eval==0.4.8)\n",
      "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.8)\n",
      "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.8)\n",
      "  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.8)\n",
      "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.8)\n",
      "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.8)\n",
      "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.8) (1.18.3)\n",
      "Collecting chardet<6,>=3.0.4 (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.8)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.8) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.8) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.8) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.8) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.8) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2018.9 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.8) (2025.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from jinja2->torch>=1.8->lm_eval==0.4.8) (3.0.2)\n",
      "Collecting click (from nltk->rouge-score>=0.0.4->lm_eval==0.4.8)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from pandas->datasets>=2.16.0->lm_eval==0.4.8) (2025.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from portalocker->sacrebleu>=1.5.0->lm_eval==0.4.8) (308)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\infinix\\desktop\\vs code\\pretraining llm\\pretrain-venv\\lib\\site-packages (from sympy->torch>=1.8->lm_eval==0.4.8) (1.3.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/11.1 MB 5.0 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.1/11.1 MB 5.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.1/11.1 MB 5.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.5/11.1 MB 5.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.8/11.1 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.1/11.1 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.2/11.1 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 6.3 MB/s eta 0:00:00\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading more_itertools-10.6.0-py3-none-any.whl (63 kB)\n",
      "Downloading numexpr-2.10.2-cp311-cp311-win_amd64.whl (144 kB)\n",
      "Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
      "Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
      "Downloading zstandard-0.23.0-cp311-cp311-win_amd64.whl (495 kB)\n",
      "Downloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
      "Downloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n",
      "Downloading scipy-1.15.2-cp311-cp311-win_amd64.whl (41.2 MB)\n",
      "   ---------------------------------------- 0.0/41.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/41.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.4/41.2 MB 5.4 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 5.0/41.2 MB 7.9 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 7.6/41.2 MB 8.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 10.0/41.2 MB 9.7 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 13.4/41.2 MB 10.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 16.5/41.2 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 19.1/41.2 MB 11.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 22.5/41.2 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 25.2/41.2 MB 11.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 28.6/41.2 MB 12.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 32.2/41.2 MB 12.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 34.9/41.2 MB 12.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 38.0/41.2 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.4/41.2 MB 12.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.2/41.2 MB 12.1 MB/s eta 0:00:00\n",
      "Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading lxml-5.3.1-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 3.1/3.8 MB 14.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 10.8 MB/s eta 0:00:00\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 13.2 MB/s eta 0:00:00\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Building wheels for collected packages: lm_eval, rouge-score, sqlitedict, word2number\n",
      "  Building wheel for lm_eval (pyproject.toml): started\n",
      "  Building wheel for lm_eval (pyproject.toml): still running...\n",
      "  Building wheel for lm_eval (pyproject.toml): still running...\n",
      "  Building wheel for lm_eval (pyproject.toml): still running...\n",
      "  Building wheel for lm_eval (pyproject.toml): still running...\n",
      "  Building wheel for lm_eval (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for lm_eval: filename=lm_eval-0.4.8-py3-none-any.whl size=3983276 sha256=9f9111fcf41b996f049a56dcc265259c588537c6300afb1c89e287371c2e858e\n",
      "  Stored in directory: C:\\Users\\Infinix\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-b45ungku\\wheels\\1a\\a8\\64\\595bbe908ea9add178046b326d8fcd3f51f1855461cca66285\n",
      "  Building wheel for rouge-score (setup.py): started\n",
      "  Building wheel for rouge-score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24972 sha256=7aa1b79b63d6e3c71a07fc892aa3669bf29bdb713e789878d1f0f8235196d254\n",
      "  Stored in directory: c:\\users\\infinix\\appdata\\local\\pip\\cache\\wheels\\1e\\19\\43\\8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "  Building wheel for sqlitedict (setup.py): started\n",
      "  Building wheel for sqlitedict (setup.py): finished with status 'done'\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16902 sha256=7713ff781cbe16e65fe99a65128d6b4ca45f4c0e0ad845a06bce72dc324a94b6\n",
      "  Stored in directory: c:\\users\\infinix\\appdata\\local\\pip\\cache\\wheels\\73\\63\\89\\7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\n",
      "  Building wheel for word2number (setup.py): started\n",
      "  Building wheel for word2number (setup.py): finished with status 'done'\n",
      "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5589 sha256=faa677eade3929320268ec8784feb026fa221c0ed1c9b2508ef2925004548fe8\n",
      "  Stored in directory: c:\\users\\infinix\\appdata\\local\\pip\\cache\\wheels\\cd\\ef\\ae\\073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n",
      "Successfully built lm_eval rouge-score sqlitedict word2number\n",
      "Installing collected packages: word2number, sqlitedict, zstandard, threadpoolctl, tcolorpy, tabulate, scipy, portalocker, pathvalidate, numexpr, more_itertools, lxml, jsonlines, joblib, click, chardet, absl-py, tqdm-multiprocess, scikit-learn, sacrebleu, nltk, mbstrdecoder, typepy, rouge-score, peft, evaluate, DataProperty, tabledata, pytablewriter, lm_eval\n",
      "Successfully installed DataProperty-1.1.0 absl-py-2.1.0 chardet-5.2.0 click-8.1.8 evaluate-0.4.3 joblib-1.4.2 jsonlines-4.0.0 lm_eval-0.4.8 lxml-5.3.1 mbstrdecoder-1.1.4 more_itertools-10.6.0 nltk-3.9.1 numexpr-2.10.2 pathvalidate-3.2.3 peft-0.14.0 portalocker-3.1.1 pytablewriter-1.2.1 rouge-score-0.1.2 sacrebleu-2.5.1 scikit-learn-1.6.1 scipy-1.15.2 sqlitedict-2.1.0 tabledata-1.3.4 tabulate-0.9.0 tcolorpy-0.1.7 threadpoolctl-3.5.0 tqdm-multiprocess-0.0.11 typepy-1.3.4 word2number-1.1 zstandard-0.23.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/EleutherAI/lm-evaluation-harness 'C:\\Users\\Infinix\\AppData\\Local\\Temp\\pip-req-build-uj1xxbsc'\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -U git+https://github.com/EleutherAI/lm-evaluation-harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf (pretrained=data\\TinySolar-308m-4k-init), gen_kwargs: (None), limit: 5.0, num_fewshot: None, batch_size: 1\n",
      "|    Tasks     |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|--------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
      "|truthfulqa_mc2|      3|none  |     0|acc   |↑  |0.4002|±  |0.2448|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13:00:01:45,293 WARNING  [lm_eval.__main__:316]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2025-03-13:00:01:45,300 INFO     [lm_eval.__main__:379] Selected Tasks: ['truthfulqa_mc2']\n",
      "2025-03-13:00:01:45,302 INFO     [lm_eval.evaluator:177] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-03-13:00:01:45,302 INFO     [lm_eval.evaluator:214] Initializing hf model, with arguments: {'pretrained': 'data\\\\TinySolar-308m-4k-init'}\n",
      "2025-03-13:00:01:45,308 INFO     [lm_eval.models.huggingface:136] Using device 'cpu'\n",
      "2025-03-13:00:01:45,722 INFO     [lm_eval.models.huggingface:377] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cpu'}\n",
      "\n",
      "Downloading readme:   0%|          | 0.00/9.59k [00:00<?, ?B/s]\n",
      "Downloading readme: 100%|██████████| 9.59k/9.59k [00:00<00:00, 9.60MB/s]\n",
      "\n",
      "Downloading data:   0%|          | 0.00/271k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████| 271k/271k [00:02<00:00, 135kB/s]\n",
      "Downloading data: 100%|██████████| 271k/271k [00:02<00:00, 135kB/s]\n",
      "\n",
      "Generating validation split:   0%|          | 0/817 [00:00<?, ? examples/s]\n",
      "Generating validation split: 100%|██████████| 817/817 [00:00<00:00, 47868.28 examples/s]\n",
      "2025-03-13:00:02:23,339 INFO     [lm_eval.api.task:420] Building contexts for truthfulqa_mc2 on rank 0...\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 406.02it/s]\n",
      "2025-03-13:00:02:23,353 INFO     [lm_eval.evaluator:525] Running loglikelihood requests\n",
      "\n",
      "Running loglikelihood requests:   0%|          | 0/33 [00:00<?, ?it/s]\n",
      "Running loglikelihood requests:   3%|▎         | 1/33 [01:24<44:52, 84.13s/it]\n",
      "Running loglikelihood requests:   6%|▌         | 2/33 [02:43<42:09, 81.60s/it]\n",
      "Running loglikelihood requests:   9%|▉         | 3/33 [03:59<39:30, 79.02s/it]\n",
      "Running loglikelihood requests:  12%|█▏        | 4/33 [05:46<43:24, 89.82s/it]\n",
      "Running loglikelihood requests:  15%|█▌        | 5/33 [07:06<40:16, 86.31s/it]\n",
      "Running loglikelihood requests:  18%|█▊        | 6/33 [08:55<42:16, 93.94s/it]\n",
      "Running loglikelihood requests:  21%|██        | 7/33 [10:38<42:06, 97.18s/it]\n",
      "Running loglikelihood requests:  24%|██▍       | 8/33 [12:21<41:09, 98.78s/it]\n",
      "Running loglikelihood requests:  27%|██▋       | 9/33 [14:00<39:33, 98.90s/it]\n",
      "Running loglikelihood requests:  30%|███       | 10/33 [15:47<38:51, 101.35s/it]\n",
      "Running loglikelihood requests:  33%|███▎      | 11/33 [17:35<37:55, 103.42s/it]\n",
      "Running loglikelihood requests:  36%|███▋      | 12/33 [19:07<34:57, 99.88s/it] \n",
      "Running loglikelihood requests:  39%|███▉      | 13/33 [20:23<30:53, 92.69s/it]\n",
      "Running loglikelihood requests:  42%|████▏     | 14/33 [21:39<27:48, 87.81s/it]\n",
      "Running loglikelihood requests:  45%|████▌     | 15/33 [22:55<25:13, 84.11s/it]\n",
      "Running loglikelihood requests:  48%|████▊     | 16/33 [24:10<23:04, 81.41s/it]\n",
      "Running loglikelihood requests:  52%|█████▏    | 17/33 [25:26<21:15, 79.72s/it]\n",
      "Running loglikelihood requests:  55%|█████▍    | 18/33 [26:41<19:36, 78.42s/it]\n",
      "Running loglikelihood requests:  58%|█████▊    | 19/33 [27:56<18:04, 77.49s/it]\n",
      "Running loglikelihood requests:  61%|██████    | 20/33 [29:11<16:37, 76.73s/it]\n",
      "Running loglikelihood requests:  64%|██████▎   | 21/33 [30:26<15:14, 76.17s/it]\n",
      "Running loglikelihood requests:  67%|██████▋   | 22/33 [31:42<13:55, 76.00s/it]\n",
      "Running loglikelihood requests:  70%|██████▉   | 23/33 [32:57<12:37, 75.75s/it]\n",
      "Running loglikelihood requests:  73%|███████▎  | 24/33 [34:12<11:19, 75.53s/it]\n",
      "Running loglikelihood requests:  76%|███████▌  | 25/33 [35:26<10:00, 75.05s/it]\n",
      "Running loglikelihood requests:  79%|███████▉  | 26/33 [36:40<08:43, 74.80s/it]\n",
      "Running loglikelihood requests:  82%|████████▏ | 27/33 [37:54<07:27, 74.54s/it]\n",
      "Running loglikelihood requests:  85%|████████▍ | 28/33 [39:09<06:13, 74.65s/it]\n",
      "Running loglikelihood requests:  88%|████████▊ | 29/33 [40:23<04:57, 74.34s/it]\n",
      "Running loglikelihood requests:  91%|█████████ | 30/33 [41:36<03:42, 74.01s/it]\n",
      "Running loglikelihood requests:  94%|█████████▍| 31/33 [42:49<02:27, 73.81s/it]\n",
      "Running loglikelihood requests:  97%|█████████▋| 32/33 [44:02<01:13, 73.42s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 33/33 [45:15<00:00, 73.26s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 33/33 [45:15<00:00, 82.28s/it]\n",
      "2025-03-13:00:47:43,395 INFO     [lm_eval.loggers.evaluation_tracker:272] Output path not provided, skipping saving results aggregated\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=data\\TinySolar-308m-4k-init \\\n",
    "    --tasks truthfulqa_mc2 \\\n",
    "    --device cpu \\\n",
    "    --limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation for the Hugging Face Leaderboard\n",
    "You can use the code below to test your own model against the evaluations required for the [Hugging Face leaderboard](https://huggingface.co/open-llm-leaderboard). \n",
    "\n",
    "If you decide to run this evaluation on your own model, don't change the few-shot numbers below - they are set by the rules of the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def h6_open_llm_leaderboard(model_name):\n",
    "  task_and_shot = [\n",
    "      ('arc_challenge', 25),\n",
    "      ('hellaswag', 10),\n",
    "      ('mmlu', 5),\n",
    "      ('truthfulqa_mc2', 0),\n",
    "      ('winogrande', 5),\n",
    "      ('gsm8k', 5)\n",
    "  ]\n",
    "\n",
    "  for task, fewshot in task_and_shot:\n",
    "    eval_cmd = f\"\"\"\n",
    "    lm_eval --model hf \\\n",
    "        --model_args pretrained={model_name} \\\n",
    "        --tasks {task} \\\n",
    "        --device cpu \\\n",
    "        --num_fewshot {fewshot}\n",
    "    \"\"\"\n",
    "    os.system(eval_cmd)\n",
    "\n",
    "h6_open_llm_leaderboard(model_name=\"YOUR_MODEL\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretrain-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
